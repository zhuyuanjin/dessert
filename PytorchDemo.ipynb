{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now the gradients of a is tensor([ 0.2500,  0.2500,  0.2500,  0.2500])\n",
      "Now the value of a is tensor([ 1.,  1.,  1.,  1.]), b is tensor([ 1.,  1.,  1.,  1.])\n",
      "Now the gradients of a is tensor([ 0.2500,  0.2500,  0.2500,  0.2500]), b is tensor([ 0.2500,  0.2500,  0.2500,  0.2500])\n",
      "Now the gradients of a is tensor([ 0.5000,  0.5000,  0.5000,  0.5000]), b is tensor([ 0.2500,  0.2500,  0.2500,  0.2500])\n",
      "Now the value of a is tensor([ 0.5000,  0.5000,  0.5000,  0.5000]), b is tensor([ 0.7500,  0.7500,  0.7500,  0.7500])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(4, requires_grad=True)\n",
    "b = torch.ones(4, requires_grad=True)\n",
    "w = a.mean()\n",
    "y = b.mean()\n",
    "z = a.mean() + b.mean().detach()\n",
    "opt1 = torch.optim.SGD([a], lr=1)\n",
    "opt2 = torch.optim.SGD([b], lr=1)\n",
    "opt3 = torch.optim.SGD([a, b], lr=1)\n",
    "w.backward()\n",
    "print('Now the gradients of a is {}'.format(a.grad))\n",
    "opt2.step()\n",
    "print('Now the value of a is {}, b is {}'.format(a, b))\n",
    "y.backward()\n",
    "print('Now the gradients of a is {}, b is {}'.format(a.grad, b.grad))\n",
    "z.backward()\n",
    "print('Now the gradients of a is {}, b is {}'.format(a.grad, b.grad))\n",
    "opt3.step()\n",
    "print('Now the value of a is {}, b is {}'.format(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  1.,  1.,  1.]])\n",
      "tensor([[ 0.1665,  0.4128, -0.0153,  0.1954]])\n",
      "tensor([[-0.8335, -0.5872, -1.0153, -0.8046]])\n",
      "tensor([[-1.8335, -1.5872, -2.0153, -1.8046]])\n",
      "tensor([[-2.8335, -2.5872, -3.0153, -2.8046]])\n"
     ]
    }
   ],
   "source": [
    "net = nn.Linear(4,1)\n",
    "opt = torch.optim.SGD(net.parameters(), lr=1)\n",
    "a = torch.ones(4)\n",
    "z = net(a)\n",
    "z.backward()\n",
    "print(net.weight.grad)\n",
    "print(net.weight.data)\n",
    "opt.step()\n",
    "print(net.weight.data)\n",
    "opt.step()\n",
    "print(net.weight.data)\n",
    "opt2 = torch.optim.SGD(net.parameters(), lr=1)\n",
    "opt2.step()\n",
    "print(net.weight.data)\n",
    "opt2.zer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "试验一下backward之后节点还在不在"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.2553])\n"
     ]
    }
   ],
   "source": [
    "net = nn.Linear(4,1)\n",
    "a = torch.ones(4)\n",
    "z = net(a)\n",
    "z.backward()\n",
    "print(z) #发现z还在 只不过不能再次求导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于同一个graph, 如果两次backward的路径不交叉，则可以backward两次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = torch.ones(4)\n",
    "b = torch.ones(4)\n",
    "net1 = nn.Linear(4,1)\n",
    "net2 = nn.Linear(4,1)\n",
    "u = net(a)\n",
    "v = net(b)\n",
    "y = u + v.detach()\n",
    "z = u.detach() + v\n",
    "y.backward()\n",
    "z.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但是如果两次求导的路径是交叉的，则不可以backward两次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-111-29e0db56b9be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "a = torch.ones(4)\n",
    "b = torch.ones(4)\n",
    "net1 = nn.Linear(4,1)\n",
    "net2 = nn.Linear(4,1)\n",
    "u = net(a)\n",
    "v = net(b)\n",
    "y = u + v.detach()\n",
    "z = u + v\n",
    "y.backward()\n",
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "探求retain_graph=True的情形,使用retain_graph=True 即使在两次backward的过程之间使用了opt.step()导致net的参数改变，第二次backward还是基于net没改变之前计算导数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4.2079])\n",
      "tensor([[ 4.2079,  4.2079,  4.2079,  4.2079]])\n",
      "tensor([ 4.2079])\n"
     ]
    }
   ],
   "source": [
    "net = nn.Linear(4,1)\n",
    "a = torch.ones(4)\n",
    "w = net(a) * net(a)\n",
    "print(2 * net(a))\n",
    "opt = torch.optim.SGD(net.parameters(), lr=0.01)\n",
    "w.backward(retain_graph=True)\n",
    "for param in net.parameters():\n",
    "    print(param.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt.step()\n",
    "opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 3.7871])\n",
      "tensor([[ 4.2079,  4.2079,  4.2079,  4.2079]])\n",
      "tensor([ 4.2079])\n"
     ]
    }
   ],
   "source": [
    "w.backward()\n",
    "print(2 * net(a))\n",
    "for param in net.parameters():\n",
    "    print(param.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 3.7871])\n",
      "tensor([[ 3.7871,  3.7871,  3.7871,  3.7871]])\n",
      "tensor([ 3.7871])\n"
     ]
    }
   ],
   "source": [
    "opt.zero_grad()\n",
    "a = torch.ones(4, requires_grad=True)\n",
    "w = net(a) * net(a)\n",
    "print(2 * net(a))\n",
    "w.backward(retain_graph=True)\n",
    "for param in net.parameters():\n",
    "    print(param.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 3.4084])\n",
      "tensor([[ 3.7871,  3.7871,  3.7871,  3.7871]])\n",
      "tensor([ 3.7871])\n"
     ]
    }
   ],
   "source": [
    "opt.step()\n",
    "opt.zero_grad()\n",
    "print(2 * net(a))\n",
    "w.backward()\n",
    "for param in net.parameters():\n",
    "    print(param.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
